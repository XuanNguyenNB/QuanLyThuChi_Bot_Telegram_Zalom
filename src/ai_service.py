"""
AI Service using Google Gemini for smart parsing and category detection.
"""

import asyncio
import json
import logging
import os
from dataclasses import dataclass
from typing import List, Optional

import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

# Configure Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Available categories for AI to choose from
CATEGORIES = [
    # Chi ti√™u - sinh ho·∫°t
    "Ch·ª£/Si√™u th·ªã",
    "ƒÇn u·ªëng", 
    "Di chuy·ªÉn",
    # Chi ph√≠ ph√°t sinh
    "Cho vay",
    "Mua s·∫Øm",
    "Gi·∫£i tr√≠",
    "L√†m ƒë·∫πp",
    "S·ª©c kh·ªèe",
    "T·ª´ thi·ªán",
    # Chi ph√≠ c·ªë ƒë·ªãnh
    "H√≥a ƒë∆°n",
    "Ng∆∞·ªùi th√¢n",
    # ƒê·∫ßu t∆∞ - ti·∫øt ki·ªám
    "ƒê·∫ßu t∆∞",
    "H·ªçc t·∫≠p",
    # Thu nh·∫≠p
    "L∆∞∆°ng",
    "Th∆∞·ªüng",
    "Thu kh√°c",
    # Kh√°c
    "Kh√°c"
]

SYSTEM_PROMPT = """B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√¢n t√≠ch chi ti√™u th√¥ng minh. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t th√¥ng tin giao d·ªãch t√†i ch√≠nh t·ª´ tin nh·∫Øn ti·∫øng Vi·ªát t·ª± nhi√™n.

QUAN TR·ªåNG - Quy t·∫Øc parse:
1. S·ªë ti·ªÅn c√≥ th·ªÉ ·ªü B·∫§T K·ª≤ v·ªã tr√≠ n√†o trong c√¢u (ƒë·∫ßu, gi·ªØa, cu·ªëi)
2. H·∫≠u t·ªë ti·ªÅn: k/K = ngh√¨n (x1000), tr/m/M = tri·ªáu (x1000000), ƒë/d/dong = ƒë∆°n v·ªã
3. S·ªë ti·ªÅn c√≥ th·ªÉ vi·∫øt: "20k", "20K", "20 ngh√¨n", "20000", "20.000"
4. **QUAN TR·ªåNG**: N·∫øu s·ªë ti·ªÅn KH√îNG c√≥ h·∫≠u t·ªë v√† < 1000, m·∫∑c ƒë·ªãnh l√† NGH√åN ƒê·ªíNG
   - "350" = 350,000ƒë (350k), "80" = 80,000ƒë (80k), "15" = 15,000ƒë (15k)
   - V√¨ ·ªü Vi·ªát Nam kh√¥ng ai d√πng 350 ƒë·ªìng, 80 ƒë·ªìng n·ªØa
5. N·∫øu c√≥ nhi·ªÅu giao d·ªãch, t√°ch th√†nh nhi·ªÅu items
6. N·∫øu c√≥ ph√©p t√≠nh (chia ƒë√¥i, chia 3, /2, tr·ª´ v·ªën...), t√≠nh to√°n s·ªë ti·ªÅn th·ª±c t·∫ø
7. M·∫∑c ƒë·ªãnh l√† CHI (expense), ch·ªâ THU (income) n·∫øu r√µ r√†ng l√† thu nh·∫≠p (b√°n, nh·∫≠n, l∆∞∆°ng...)
8. "tr·ª´ v·ªën X" nghƒ©a l√†: s·ªë ti·ªÅn nh·∫≠n - X = l·ª£i nhu·∫≠n th·ª±c

Danh m·ª•c c√≥ s·∫µn: """ + ", ".join(CATEGORIES) + """

Tr·∫£ v·ªÅ JSON:
{
  "transactions": [
    {
      "amount": <s·ªë ti·ªÅn ƒë√£ t√≠nh, ki·ªÉu number>,
      "note": "<m√¥ t·∫£ ng·∫Øn g·ªçn>",
      "category": "<t√™n danh m·ª•c ph√π h·ª£p nh·∫•t>",
      "type": "expense" ho·∫∑c "income"
    }
  ],
  "understood": true/false,
  "message": "<l√Ω do n·∫øu kh√¥ng hi·ªÉu>"
}

V√ç D·ª§ PARSE:
- "mua b√°nh m√¨ 20k" -> amount=20000, note="mua b√°nh m√¨", category="ƒÇn u·ªëng"
- "20k b√°nh m√¨" -> amount=20000, note="b√°nh m√¨", category="ƒÇn u·ªëng"  
- "cafe 50" -> amount=50000, note="cafe", category="ƒÇn u·ªëng" (50 = 50k)
- "ƒë·ªï xƒÉng 100" -> amount=100000, note="ƒë·ªï xƒÉng", category="Di chuy·ªÉn" (100 = 100k)
- "grab 35" -> amount=35000, note="grab", category="Di chuy·ªÉn" (35 = 35k)
- "si√™u th·ªã 500" -> amount=500000, note="si√™u th·ªã", category="Ch·ª£/Si√™u th·ªã" (500 = 500k)
- "ƒÉn tr∆∞a 150k chia ƒë√¥i" -> amount=75000, note="ƒÉn tr∆∞a", category="ƒÇn u·ªëng"
- "l∆∞∆°ng th√°ng 12 15tr" -> amount=15000000, note="l∆∞∆°ng th√°ng 12", category="L∆∞∆°ng", type="income"
- "up x7u colorvs 350 tr·ª´ v·ªën 80" -> amount=270000, note="up x7u colorvs", category="L∆∞∆°ng", type="income" (350k - 80k = 270k l·ª£i nhu·∫≠n)
- "b√°n g√≥i gpt plus 50" -> amount=50000, note="b√°n g√≥i gpt plus", category="L∆∞∆°ng", type="income"

N·∫æU KH√îNG T√åM TH·∫§Y S·ªê TI·ªÄN trong tin nh·∫Øn -> understood=false
"""


@dataclass
class AITransaction:
    """Parsed transaction from AI"""
    amount: float
    note: str
    category: str
    type: str  # "expense" or "income"


@dataclass
class AIParseResult:
    """Result from AI parsing"""
    transactions: List[AITransaction]
    understood: bool
    message: Optional[str] = None
    raw_response: Optional[str] = None


def is_ai_enabled() -> bool:
    """Check if AI service is configured"""
    return bool(GEMINI_API_KEY)


async def parse_with_ai(text: str) -> AIParseResult:
    """
    Use Gemini AI to parse user message into transactions.
    
    Args:
        text: Raw user message
        
    Returns:
        AIParseResult with parsed transactions
    """
    if not is_ai_enabled():
        return AIParseResult(
            transactions=[],
            understood=False,
            message="AI ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh"
        )
    
    try:
        # Combine system prompt with user message
        full_prompt = f"""{SYSTEM_PROMPT}

---

Ph√¢n t√≠ch tin nh·∫Øn chi ti√™u sau v√† tr·∫£ v·ªÅ JSON:

Tin nh·∫Øn: "{text}"

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch th√™m."""

        # Use sync API with asyncio.to_thread to avoid event loop conflicts
        def _sync_generate():
            model = genai.GenerativeModel('gemini-2.0-flash')
            return model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=500,
                )
            )
        
        response = await asyncio.to_thread(_sync_generate)
        
        response_text = response.text.strip()
        logger.info(f"AI response: {response_text}")
        
        # Clean up response - remove markdown code blocks if present
        if response_text.startswith("```"):
            lines = response_text.split("\n")
            # Remove first and last lines (```json and ```)
            response_text = "\n".join(lines[1:-1])
        
        # Parse JSON response
        try:
            data = json.loads(response_text)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group())
            else:
                return AIParseResult(
                    transactions=[],
                    understood=False,
                    message="Kh√¥ng th·ªÉ parse response t·ª´ AI",
                    raw_response=response_text
                )
        
        # Convert to AITransaction objects
        transactions = []
        for tx in data.get("transactions", []):
            transactions.append(AITransaction(
                amount=float(tx.get("amount", 0)),
                note=tx.get("note", ""),
                category=tx.get("category", "Kh√°c"),
                type=tx.get("type", "expense")
            ))
        
        return AIParseResult(
            transactions=transactions,
            understood=data.get("understood", True),
            message=data.get("message"),
            raw_response=response_text
        )
        
    except Exception as e:
        logger.error(f"AI parsing error: {e}")
        return AIParseResult(
            transactions=[],
            understood=False,
            message=f"L·ªói AI: {str(e)}"
        )


def get_category_name_from_ai(ai_category: str) -> str:
    """Map AI category to actual category name"""
    # Normalize and find best match
    ai_cat_lower = ai_category.lower().strip()
    
    for cat in CATEGORIES:
        if cat.lower() == ai_cat_lower:
            return cat
    
    # Fuzzy matching
    for cat in CATEGORIES:
        if ai_cat_lower in cat.lower() or cat.lower() in ai_cat_lower:
            return cat
    
    return "Kh√°c"


def is_question(text: str) -> bool:
    """Check if text is a question rather than a transaction"""
    text_lower = text.lower().strip()
    
    # Question patterns
    question_words = [
        'bao nhi√™u', 'm·∫•y', 'sao', 't·∫°i sao', 'nh∆∞ th·∫ø n√†o', 'th·∫ø n√†o',
        '·ªü ƒë√¢u', 'khi n√†o', 'ai', 'g√¨', 'c√°i g√¨', 'l√† g√¨',
        'c√≥ th·ªÉ', 'l√†m sao', 'gi√∫p', 'h·ªèi', 'cho h·ªèi',
        'th√°ng n√†y', 'h√¥m nay', 'tu·∫ßn n√†y', 'chi ti√™u',
        't·ªïng', 'trung b√¨nh', 'nhi·ªÅu nh·∫•t', '√≠t nh·∫•t'
    ]
    
    # Check if starts with question word or contains question mark
    if text.endswith('?'):
        return True
    
    for word in question_words:
        if word in text_lower:
            return True
    
    return False


@dataclass
class QueryIntent:
    """Parsed query intent from natural language"""
    is_query: bool = False
    time_range: str = "all"  # today, week, month, year, all
    category: Optional[str] = None
    keyword: Optional[str] = None


async def parse_query_intent(text: str) -> QueryIntent:
    """Use AI to parse a natural language query about spending"""
    if not is_ai_enabled():
        return QueryIntent(is_query=False)
    
    try:
        prompt = f"""Ph√¢n t√≠ch c√¢u h·ªèi v·ªÅ chi ti√™u sau v√† tr·∫£ v·ªÅ JSON.

C√¢u h·ªèi: "{text}"

Tr·∫£ v·ªÅ JSON v·ªõi format:
{{
    "is_query": true/false,  // true n·∫øu ƒë√¢y l√† c√¢u h·ªèi v·ªÅ th·ªëng k√™/t·ªïng ti·ªÅn
    "time_range": "today" | "week" | "month" | "year" | "all",
    "category": "t√™n danh m·ª•c n·∫øu c√≥" | null,
    "keyword": "t·ª´ kh√≥a t√¨m trong ghi ch√∫" | null
}}

V√≠ d·ª•:
- "th√°ng n√†y cho ng∆∞·ªùi y√™u bao nhi√™u" ‚Üí {{"is_query": true, "time_range": "month", "category": "Ng∆∞·ªùi th√¢n", "keyword": "ng∆∞·ªùi y√™u"}}
- "tu·∫ßn n√†y cafe bao nhi√™u" ‚Üí {{"is_query": true, "time_range": "week", "category": "ƒÇn u·ªëng", "keyword": "cafe"}}
- "nƒÉm nay chi bao nhi√™u" ‚Üí {{"is_query": true, "time_range": "year", "category": null, "keyword": null}}
- "t·ª´ ƒë·∫ßu t·ªõi gi·ªù cho b·ªë m·∫π bao nhi√™u" ‚Üí {{"is_query": true, "time_range": "all", "category": "Ng∆∞·ªùi th√¢n", "keyword": "b·ªë m·∫π"}}
- "h√¥m nay ti√™u g√¨ v·∫≠y" ‚Üí {{"is_query": true, "time_range": "today", "category": null, "keyword": null}}

Danh m·ª•c c√≥ s·∫µn: {', '.join(CATEGORIES)}

CH·ªà tr·∫£ v·ªÅ JSON, kh√¥ng gi·∫£i th√≠ch."""

        def _sync_generate():
            model = genai.GenerativeModel('gemini-2.0-flash')
            return model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=200,
                )
            )
        
        response = await asyncio.to_thread(_sync_generate)
        response_text = response.text.strip()
        # Clean up markdown if present
        if response_text.startswith("```"):
            response_text = response_text.split("```")[1]
            if response_text.startswith("json"):
                response_text = response_text[4:]
        response_text = response_text.strip()
        
        data = json.loads(response_text)
        
        return QueryIntent(
            is_query=data.get("is_query", False),
            time_range=data.get("time_range", "all"),
            category=data.get("category"),
            keyword=data.get("keyword")
        )
        
    except Exception as e:
        logger.error(f"AI query parse error: {e}")
        return QueryIntent(is_query=False)


async def answer_question(text: str, spending_context: str = "") -> str:
    """Use AI to answer a natural language question about spending"""
    if not is_ai_enabled():
        return "AI ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng th·ª≠ l·∫°i sau."
    
    try:
        qa_prompt = f"""B·∫°n l√† tr·ª£ l√Ω t√†i ch√≠nh c√° nh√¢n th√¥ng minh. Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ng·∫Øn g·ªçn v√† h·ªØu √≠ch.

D·ªØ li·ªáu chi ti√™u c·ªßa ng∆∞·ªùi d√πng:
{spending_context}

C√¢u h·ªèi: "{text}"

Quy t·∫Øc:
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán
- D√πng s·ªë li·ªáu c·ª• th·ªÉ n·∫øu c√≥
- ƒê∆∞a ra g·ª£i √Ω thi·∫øt th·ª±c
- N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, h√£y n√≥i r√µ
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát"""

        def _sync_generate():
            model = genai.GenerativeModel('gemini-2.0-flash')
            return model.generate_content(
                qa_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=500,
                )
            )
        
        response = await asyncio.to_thread(_sync_generate)
        return response.text.strip()
        
    except Exception as e:
        logger.error(f"AI Q&A error: {e}")
        return f"Xin l·ªói, m√¨nh kh√¥ng th·ªÉ tr·∫£ l·ªùi l√∫c n√†y. H√£y th·ª≠ l·∫°i sau nh√©!"


async def generate_transaction_comment(amount: float, note: str, category: str, tx_type: str = "expense") -> str:
    """Generate a fun/engaging comment for a transaction"""
    if not is_ai_enabled():
        return ""
    
    try:
        type_text = "thu nh·∫≠p" if tx_type == "income" else "chi ti√™u"
        
        prompt = f"""T·∫°o m·ªôt c√¢u b√¨nh lu·∫≠n ng·∫Øn, vui v·∫ª v·ªÅ giao d·ªãch sau:
- Lo·∫°i: {type_text}
- S·ªë ti·ªÅn: {amount:,.0f}ƒë
- M√¥ t·∫£: {note}
- Danh m·ª•c: {category}

Quy t·∫Øc:
- Ch·ªâ 1 c√¢u ng·∫Øn (d∆∞·ªõi 15 t·ª´)
- Vui v·∫ª, th√¢n thi·ªán, c√≥ th·ªÉ h√†i h∆∞·ªõc nh·∫π
- D√πng 1-2 emoji ph√π h·ª£p
- N·∫øu l√† thu nh·∫≠p: ch√∫c m·ª´ng, ƒë·ªông vi√™n
- N·∫øu l√† chi ti√™u: nh·∫≠n x√©t nh·∫π nh√†ng, kh√¥ng ph√°n x√©t
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- CH·ªà tr·∫£ v·ªÅ c√¢u b√¨nh lu·∫≠n, kh√¥ng gi·∫£i th√≠ch"""

        def _sync_generate():
            model = genai.GenerativeModel('gemini-2.0-flash')
            return model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.9,
                    max_output_tokens=50,
                )
            )
        
        response = await asyncio.to_thread(_sync_generate)
        return response.text.strip()
        
    except Exception as e:
        logger.error(f"AI comment error: {e}")
        return ""


async def transcribe_voice(audio_bytes: bytes) -> Optional[str]:
    """Transcribe voice message to text using Gemini"""
    if not is_ai_enabled():
        return None
    
    try:
        # Upload audio data
        audio_part = {
            "mime_type": "audio/ogg",
            "data": audio_bytes
        }
        
        prompt = """Chuy·ªÉn ƒëo·∫°n ghi √¢m n√†y th√†nh vƒÉn b·∫£n ti·∫øng Vi·ªát.
Ch·ªâ tr·∫£ v·ªÅ vƒÉn b·∫£n ƒë∆∞·ª£c n√≥i, kh√¥ng th√™m g√¨ kh√°c.
N·∫øu kh√¥ng nghe r√µ ho·∫∑c kh√¥ng c√≥ ti·∫øng n√≥i, tr·∫£ v·ªÅ: [kh√¥ng nghe r√µ]"""
        
        def _sync_generate():
            model = genai.GenerativeModel('gemini-2.0-flash')
            return model.generate_content(
                [prompt, audio_part],
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=500,
                )
            )
        
        response = await asyncio.to_thread(_sync_generate)
        text = response.text.strip()
        if text and text != "[kh√¥ng nghe r√µ]":
            return text
        return None
        
    except Exception as e:
        logger.error(f"Voice transcription error: {e}")
        return None


async def chat_casual(text: str) -> str:
    """Use AI for casual conversation when message is not about transactions"""
    if not is_ai_enabled():
        return "Ch√†o b·∫°n! M√¨nh l√† bot ghi ch√©p chi ti√™u. G√µ nh∆∞: `cafe 50` ƒë·ªÉ ghi chi ti√™u nh√©!"
    
    try:
        chat_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω ghi ch√©p chi ti√™u th√¢n thi·ªán.

Ng∆∞·ªùi d√πng v·ª´a nh·∫Øn: "{text}"

ƒê√¢y KH√îNG ph·∫£i l√† tin nh·∫Øn v·ªÅ chi ti√™u/thu nh·∫≠p. H√£y tr·∫£ l·ªùi th√¢n thi·ªán, ng·∫Øn g·ªçn.

Quy t·∫Øc:
- Tr·∫£ l·ªùi t·ª± nhi√™n, vui v·∫ª nh∆∞ b·∫°n b√®
- Ng·∫Øn g·ªçn (1-2 c√¢u)
- C√≥ th·ªÉ d√πng emoji
- N·∫øu ph√π h·ª£p, nh·∫Øc nh·∫π v·ªÅ ch·ª©c nƒÉng ghi chi ti√™u
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- KH√îNG tr·∫£ l·ªùi c√°c c√¢u h·ªèi nh·∫°y c·∫£m/kh√¥ng ph√π h·ª£p"""

        def _sync_generate():
            model = genai.GenerativeModel('gemini-2.0-flash')
            return model.generate_content(
                chat_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.8,
                    max_output_tokens=150,
                )
            )
        
        response = await asyncio.to_thread(_sync_generate)
        return response.text.strip()
        
    except Exception as e:
        logger.error(f"AI chat error: {e}")
        return "Ch√†o b·∫°n! üëã M√¨nh l√† bot ghi chi ti√™u. G√µ nh∆∞: `cafe 50` nh√©!"
